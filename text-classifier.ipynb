{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "boyValkI2fpk"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "import nltk\n",
    "from collections import Counter\n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk.tokenize import word_tokenize \n",
    "import pickle\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Hil-g2AM2iT9"
   },
   "outputs": [],
   "source": [
    "# Creates dictionary from all the emails in the directory\n",
    "def build_dictionary(dir):\n",
    "  # Read the file names\n",
    "  emails = os.listdir(dir)\n",
    "  emails.sort()\n",
    "  # Array to hold all the words in the emails\n",
    "  dictionary = []\n",
    "\n",
    "  # Collecting all words from those emails\n",
    "  for email in emails:\n",
    "    m = open(os.path.join(dir, email))\n",
    "    for i, line in enumerate(m):\n",
    "      if i == 2: # Body of email is only 3rd line of text file\n",
    "        words = line.split()\n",
    "        dictionary += words\n",
    "\n",
    "  # We now have the array of words, whoch may have duplicate entries\n",
    "  dictionary = list(set(dictionary)) # Removes duplicates\n",
    "\n",
    "  # Removes puctuations and non alphabets\n",
    "  for index, word in enumerate(dictionary):\n",
    "    if (word.isalpha() == False) or (len(word) == 1):\n",
    "      del dictionary[index]\n",
    "   #remove stopwords \n",
    "  for count,word in enumerate(dictionary):\n",
    "    if word in stop_words:\n",
    "        del dictionary[count]   \n",
    "  dictionar_stem=[]\n",
    "  for w in dictionary:\n",
    "        dictionar_stem.append(lemmatizer.lemmatize(w.lower()))  \n",
    "#   dictionary = Counter(dictionar_stem)\n",
    "#   dictionary = dictionary.most_common(most)\n",
    "  \n",
    "\n",
    "  return dictionar_stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8slxPd4J2iRf"
   },
   "outputs": [],
   "source": [
    "def build_features(dir, dictionary):\n",
    "  # Read the file names\n",
    "  emails = os.listdir(dir)\n",
    "  emails.sort()\n",
    "  # ndarray to have the features\n",
    "  features_matrix = np.zeros((len(emails), len(dictionary)))\n",
    "\n",
    "  # collecting the number of occurances of each of the words in the emails\n",
    "  for email_index, email in enumerate(emails):\n",
    "    m = open(os.path.join(dir, email))\n",
    "    for line_index, line in enumerate(m):\n",
    "      if line_index == 2: #in each email body is at third line, which make index 2\n",
    "        words = line.split()\n",
    "        for word_index, word in enumerate(dictionary):\n",
    "          features_matrix[email_index, word_index] = words.count(word)\n",
    "\n",
    "  return features_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ua-AdD5d2iOo"
   },
   "outputs": [],
   "source": [
    "def build_labels(dir):\n",
    "  # Read the file names\n",
    "  emails = os.listdir(dir)\n",
    "  emails.sort()\n",
    "  # ndarray of labels\n",
    "  labels_matrix = np.zeros(len(emails))\n",
    "\n",
    "  for index, email in enumerate(emails):\n",
    "    labels_matrix[index] = 1 if re.search('spms*', email) else 0\n",
    "\n",
    "  return labels_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "f0Ry0gQ22iMF",
    "outputId": "04ee3265-8fd7-4b04-bf9f-340c9f8862d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Building dictionary\n",
      "[1, 2, 3, 4, 5, 6, 7, 9]\n",
      "dictionary length 45591\n"
     ]
    }
   ],
   "source": [
    "#choose 9 folder for traing and the remaining one for testing\n",
    "files=list(range(1,10))\n",
    "print(files)\n",
    "print('Building dictionary')\n",
    "train_dir=[]\n",
    "test_file=files.pop(7)\n",
    "for i in files:\n",
    "        train_dir.append('../input/emaildataset/emaildataset/part{}'.format(i))\n",
    "sub_dictionary=[]\n",
    "for i in train_dir:\n",
    "    sub_dictionary.append(build_dictionary(i))\n",
    "#convert sub list (dictionary) to one flat list\n",
    "flat_list = [item for sublist in sub_dictionary for item in sublist]\n",
    "dictionary=list(set(flat_list)) #remove duplicated\n",
    "print(files)\n",
    "print('dictionary length', len(dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "NZaxy6jw5YRX",
    "outputId": "b9c4b81c-fd3a-4a83-e2f6-f58e6546dee0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45591"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "evPegAi92iJm",
    "outputId": "1138e125-8f5d-41e9-ac73-7f813f7d5c06"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building training features and labels\n",
      "CPU times: user 17min 18s, sys: 2.38 s, total: 17min 21s\n",
      "Wall time: 17min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print('Building training features and labels')\n",
    "sub_features_train=[]\n",
    "sub_labels_train=[]\n",
    "for i in train_dir:\n",
    "    sub_features_train.append(build_features(i, dictionary)) #X train\n",
    "    sub_labels_train.append(build_labels(i)) #y train\n",
    "features_train=np.concatenate(sub_features_train)\n",
    "labels_train=np.concatenate(sub_labels_train)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "dbiCqf-7-G-c",
    "outputId": "c1d42d79-b57e-4bc8-d8bf-6044da20a9d2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "sRyDCnoZ4LrP",
    "outputId": "9ce7b00e-6e2b-4d17-cc62-c243139921fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. Building the test features and labels\n"
     ]
    }
   ],
   "source": [
    "test_dir = '../input/emaildataset/emaildataset/part{}'.format(test_file)\n",
    "print('4. Building the test features and labels')\n",
    "features_test = build_features(test_dir, dictionary) #X_test\n",
    "labels_test = build_labels(test_dir) #y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3. Training the classifier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=0, solver='liblinear',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression(solver='liblinear',random_state = 0)\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# classifier = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)\n",
    "#classifier.fit(X_train, y_train)\n",
    "print('3. Training the classifier')\n",
    "classifier.fit(features_train, labels_train) #X train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "JfUQvqrqSSOq",
    "outputId": "fc5b05eb-aeb2-4fd4-e04a-6a291b8371ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5. Calculating accuracy of the trained classifier\n",
      "0.986159169550173\n"
     ]
    }
   ],
   "source": [
    "print('5. Calculating accuracy of the trained classifier')\n",
    "accuracy = classifier.score(features_test, labels_test)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "v_bX53TO-9J7",
    "outputId": "93201af2-f062-45a7-ad8a-0c7b3db9e26a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      1.00      0.99       241\n",
      "         1.0       1.00      0.92      0.96        48\n",
      "\n",
      "   micro avg       0.99      0.99      0.99       289\n",
      "   macro avg       0.99      0.96      0.97       289\n",
      "weighted avg       0.99      0.99      0.99       289\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Classification report\n",
    "label_pred = classifier.predict(features_test) # ypred #feature test ---> y test\n",
    "print(classification_report(labels_test , label_pred ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = [i for i in range(len(labels_test)) if labels_test[i] != label_pred[i]]\n",
    "wrong_predictions = features_test[indices,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('text_classifier', 'wb') as picklefile:  \n",
    "    pickle.dump(classifier,picklefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['text_classification.pkl']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "\n",
    "# Save to file in the current working directory\n",
    "joblib_file = \"text_classification.pkl\"  \n",
    "joblib.dump(classifier, joblib_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dictionary', 'wb') as f:\n",
    "    pickle.dump(dictionary, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[262, 264, 265, 267]\n"
     ]
    }
   ],
   "source": [
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../input/emaildataset/emaildataset/part8\n"
     ]
    }
   ],
   "source": [
    "print(test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "all_files = glob.glob(os.path.join(test_dir, \"*.txt\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject: analysts choose \" adlu \" microcap bestock pick\n",
      "\n",
      "* * * * * we believe in opt-in policies . if you did not request this investor email , please do not reply , you will be taken off our email list automatically , or reply with remove * * adlu - otc : bb - patented , trademarked products , top management , sec form 10 reporting compliance , big four accounting and audits , international advertising campaign , digital imaging and internet applications , good investor relations , small float . this micro - cap is doing everything right ! - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - dear investor , the analysts at bestockpix have chosen adlu as a micro - cap june ' 99 first pick . this swiss based , surface coating technology company is doing everything investors look for . the adlu consumer market - digital imagery and internet printing applications - is an 85 billion photographs per year target market and growing . research has found no manufacturing process which could compete directly with adlu 's patented , trademarked \" brightec \" alkaline rare-earth luminescent technology . add strong management , saatchi & saatchi advertising campaigns , full sec form 10 reporting compliance , atag ernst young accounting and audits , small share float and undervalued share price and you have all the companents for a bestockpix micro - cap \" first pick \" . - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - bestockpix invites you to investigate the accuracy of our research , which is developed from publicly available sources . our ' picks ' are based on the potential for significant returns , but are not an endorsement or solicitation to purchase any stock or security . to receive a full list of bestockpix monthly , for an annual fee of $ 50 u . s . , plese reply to this e-mail with \" subscribe \" in subject line . - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - bestockpix invests in ' first pick ' recomendations and currently owns shares of adlu common stock . - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - this ad is being sent in compliance with senate bill 1618 , title 3 , section 301 . http : / / www . senate . gov / ~ murkowski / commercialemail / s771index . html\n",
      "\n",
      "Subject: update , ausschreibung professur allgemeine sprachwissenschaft\n",
      "\n",
      "hinweise zur bewerbung auf eine c4 - professur fuer allgemeine sprachwissenschaft an der universitaet heidelberg : die ausschreibung ist erschienen in \" ausschreibungsdienst des deutschen hochschulverbandes \" , ausgabe 3 - i vom 12 . januar 1995 , rheinallee 18 , d-53173 bonn , telefon + 49 228 36 40 02 , fax + 49 228 35 34 03 . einsendeschluss fuer bewerbungen ist der 22 . februar 1995 . hier ein paar hinweise zur bewerbung auf eine c - 4 professur in deutschland : die bewerbung muss enthalten - ein anschreiben an den dekan der neuphilologischen fakultdt der universitaet heidelberg , hauptstrasse 120 , d-69117 heidelberg , dass man sich um die stelle bewirbt , - ein tabellarischer lebenslauf , enthaltend schulische und universitaere ausbildung , berufliche laufbahn , sprachkenntnisse , evtl . auszeichnungen und wichtige private verhaeltnisse wir heirat , kinder , - eine liste der veroeffentlichungen , - eine liste der gehaltenen lehrveranstaltungen . man kann beliebiges hinzufuegen , wenn es fuer eine gute praesentation geeignet erscheint . auch die aeussere form ist nicht direkt geregelt . offprints der schriften braucht man noch nicht einzusenden . man wird dazu eingeladen , wenn man in die engere auswahl kommt . noch ein hinweis zu qualifikation : in deutschland ist fuer diese stelle die habilitation eine voraussetzung ( d . i . ein spezieller akademischer grad zusaetzlich zur promotion ) . von auslaendern werden gleichwertige qualifikationen erwartet , d . h . in der regel , dass man schon professor sein muss , wenn man sich um diese stelle bewirbt .\n",
      "\n",
      "Subject: re : 8 . 1208 , sum : double - dutch and youthese / pig latin\n",
      "\n",
      "on thu , 21 aug 1997 , the linguist list < linguist @ linguistlist . org > wrote : > from : waruno mahdi < mahdi @ fhi-berlin . mpg . de > > subject : summary : double - dutch and youthese / pig latin > jack hall : > > in my response to the query about pig latin , i mentioned what i called > the \" op \" language , which i read about in a book or magazine when i was > about 10-12 years old ( mid 1950 's ) . as i recall , the simple rule was : > put \" op \" ( phonetically [ a : p ] after every consonant in a word except > the last ( final ) consonant . i am not certain what the rule was about > consonant clusters . thus \" dog \" would be \" dopog \" . i remember > specifically that the word \" umbrella \" was given as : > \" umopbopropellopa \" , indicating that \" op \" is to be placed after all > three consonants at the beginning ( umbr - - ) , but only one after the > double \" l \" . i have never met anybody who has heard of this language , > or knew how to use it , and , since i learned about it from a book , > rather than from other people ( children ) , i cannot say anything about > the sociolinguistics of it . for me it is an idiolect ( ! ! ) i ' ve heard of it . when i was about 10 ( 1973-74 ) some friends and i played around with a language we called \" oppish \" . we did it a little differently from what you described above . \" op \" was inserted after each consonant , even the last one , based on how the word was spelled , so that \" ship \" would be \" sophopipop \" , and \" umbrella \" would be \" umopbopropeloplopa . \" > > some tentative conclusions : > > ( a ) both phenomena , pig latin - type phonologically manipulated secret > language , and youth slang , are apparently neither an anglosaxon , > nor a european particularity . > > ( b ) predeliction to pig latin - type language game covers a much wider > age bracket than i had initially suspected , beginning at around 10 > years , and overlapping with youth slang , in which pig latin - type > expressions may be taken up as slang - specific words . i think i was familiar with pig latin as young as 5 or 6 ( of course , i had older brothers , so that helped ) , and i remember using it with friends in about the second or third grade ( 7 to 9 years old ) . pig latin is also used occasionally by adults , often to keep their very young children from understanding what they are talking about ( similar to spelling words out ) . i also remember that fred flintstone ( from the tv cartoon series \" the flintstones \" sometimes muttered , \" ix - nay , barney , ix-nay , \" when he thought that barney rubble was saying too much . that 's pig latin for \" nix , barney , nix , \" where \" nix \" ( meaning \" nothing \" ) is slang for \" shut up before you get us in trouble , \" or \" put a sock in it . \" my parents also had a spike jones christmas record album that included \" jingle bells \" sung partly in pig latin by some children : \" ingle - jay ells-bay , ingle-jay ells-bay , ingle-jay all the ay-way . . . \" kevin caldwell\n",
      "\n",
      "Subject: re : 8 . 137 , disc : low vowels in pie\n",
      "\n",
      "linguist @ linguistlist . org wrote : > subject : 8 . 137 , disc : low vowels in pie > editor for this issue : susan robinson < sue @ linguistlist . org > > > = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = directory = = = = = = = = = = = = = = = = = = > > 1 ) > date : mon , 27 jan 1997 20 : 33 : 15 + 0000 > from : \" miguel carrasquer vidal \" < mcv @ pi . net > > subject : re : 8 . 113 , sum : low vowels in pie > > if we further merge * e and * o into a pre - ablaut * * a , pre - pie still > emerges with a three vowel system ( * * a , * * i , * * u ) . there is no reason > to deny * i and * u vowelhood before the emergence of ablaut ( if there > is after ablaut ) . in conclusion : ( pre - ) pie never had a single vowel > \" phoneme \" . not only is it typologically implausible , it does not > follow from the reconstruction . i am truly surprised that the question of the original vowel quality of ie < i > and < u > can arise again and again . there is not a single good argument for not regarding ie < i > and < u > as reductions from < y > and < w > : 1 ) if < i > were an ie vowel , why is it that an ie dictionary like pokorny has an i - section with two entries ( both of which have slavic cognates in jv - ) but 35 entries under y - ? 2 ) if < u > were an ie vowel , why is it that an ie dictionary like pokorny has an u - section with eight entries ( most of which have slavic or italic cognates in vv - ) but 141 entries under w - ? 3 ) compare this to 146 beginning with a - ( he ) and 95 under e - ( he ) and 43 und o - ( he ) . 4 ) if ie < i > or < u > were original , when initial , we would have to reconstruct hi / u , the same \" laryngeal \" that , with < e > , yields ie e - , i . e . one which does not change the quality of the vowel . it is not reasonable to hi and hu the source of these ten entries ( combined ) and attribute the some vowelhood to i / u that e has ( 95 entries ) . 5 ) the 189 entries beginning with a - and o - cannot arise from * hi or * hu ( at least no one has seriously suggested this to my knowledge ) , therefore must arise from a different combinations of he under different circumstances . this gives us 284 entries for ( h ) e as against 10 entries ( combined ( h ) i - and ( h ) u - ) , a very strange distribution of vowels . 6 ) i will not bother to cite aa cognates for ie words with cvi ( c ) or cvu ( c ) because many list readers do not accept the nostratic parentage of ie and aa but for those who can entertain such a heresy , we find that ie cvi and cvu correspond to aa cvy / $ [ ain ] and cvw . 7 ) typology has been severely abused in this question . whatever old indian may have been , as we find it , it has one vowel , < a > , and every other \" vowel \" is simply derived from a + h / y / w . why ie could not have been such a language , in which the h / w / y had not yet been resolved into other vowel qualities ( a : / e : / o : / i / u , etc . ) simply escapes me . pat ryan patrick c . ryan < proto-language @ worldnet . att . net > ( 501 ) 227-9947 ; fax / data ( 501 ) 312-9947 9115 w . 34th st . * little rock , ar 72204-4441 * usa webpage : < a href = \" http : / / www . geocities . com / athens / forum / 2803 \" > < / a > * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * ' veit ek , at ek hekk , vindga meidhi , naetr allar niu , geiri undadhr . . . a theim meidhi er mangi veit hvers hann af rotum renn . ' * ( havamal 138 ) * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in indices:\n",
    "    print(open(all_files[i]).read())"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of machine learning project",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
